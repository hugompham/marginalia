/**
 * AI Prompt Building and Response Parsing
 *
 * Contains prompt templates and parsing logic for AI-generated flashcards.
 *
 * @module services/ai/prompts
 */

import type { Highlight, Collection, QuestionType, Difficulty, SourceType } from '$lib/types';
import { tryParseJSON } from './parse-utils';

/**
 * A question generated by AI before user approval
 */
export interface GeneratedQuestion {
	/** Source highlight ID */
	highlightId: string;
	/** Type of question generated */
	questionType: QuestionType;
	/** Question text */
	question: string;
	/** Answer text */
	answer: string;
	/** Cloze text with deletion markers (only for cloze type) */
	clozeText?: string;
	/** AI confidence score (0-1) */
	confidence: number;
}

/**
 * Additional context passed to prompt generation
 */
export interface PromptContext {
	/** Tags per highlight ID */
	highlightTags?: Record<string, string[]>;
	/** Number of existing cards per highlight ID */
	existingCardCounts?: Record<string, number>;
	/** Difficulty level */
	difficulty?: Difficulty;
}

/** Shared system message used by both OpenAI and Anthropic */
export const SYSTEM_MESSAGE =
	'You are an expert educational content creator specializing in spaced repetition. ' +
	'You produce questions that require active recall and test deep understanding. ' +
	"Target Bloom's taxonomy levels: Apply, Analyze, Evaluate -- not just Remember. " +
	"Match the domain's vocabulary and depth. Always output valid JSON.";

/**
 * Instructions for each question type, keyed by difficulty level.
 * 'standard' targets recall + basic understanding (Bloom's 1-2).
 * 'challenging' targets application, analysis, evaluation (Bloom's 3-5).
 */
const typeInstructions: Record<Difficulty, Record<QuestionType, string>> = {
	standard: {
		cloze: `
CLOZE DELETIONS:
- Blank a multi-word phrase that tests understanding of a relationship or mechanism, not a single obvious keyword
- The blank should have one clear, unambiguous answer derivable from the highlight
- Keep surrounding context meaningful so the card is self-contained
- Format: "The {{c1::answer phrase}} is important because..."
- Only one cloze deletion per question

DO NOT: blank the most obvious single word (e.g., blanking "neuroplasticity" in a sentence about neuroplasticity)
DO: blank a phrase that requires understanding (e.g., "Neuroplasticity allows the brain to {{c1::reorganize neural pathways based on new experiences}}")

Example (good):
  Highlight: "Compound interest means earning interest on both the original principal and the accumulated interest from previous periods."
  clozeText: "Compound interest means earning interest on {{c1::both the original principal and the accumulated interest}} from previous periods."
  answer: "both the original principal and the accumulated interest"
  confidence: 0.92

Example (bad -- too easy, blanks single obvious term):
  clozeText: "{{c1::Compound interest}} means earning interest on both the original principal and the accumulated interest."`,

		definition: `
DEFINITION QUESTIONS:
- Go beyond "What is X?" -- ask about significance, mechanism, or distinction
- Good formats: "What distinguishes X from Y?", "What role does X play in Z?", "How is X defined in the context of Y?"
- Answer should demonstrate understanding, not dictionary regurgitation
- Keep answers under 3 sentences

Example (good):
  Highlight: "Mitochondria are organelles that generate most of the cell's supply of ATP through oxidative phosphorylation."
  question: "What role do mitochondria play in cellular energy production?"
  answer: "Mitochondria generate most of the cell's ATP supply through oxidative phosphorylation, making them the primary site of aerobic energy production."
  confidence: 0.90

Example (bad -- surface-level):
  question: "What are mitochondria?"
  answer: "Organelles in cells."`,

		conceptual: `
CONCEPTUAL QUESTIONS:
- Ask causal reasoning ("Why does X lead to Y?"), implication analysis ("What would happen if X changed?"), or comparison questions ("How does X differ from Y in terms of Z?")
- The answer MUST reference specific evidence or reasoning from the highlight
- Avoid yes/no questions and questions answerable without the source material

Example (good):
  Highlight: "Inflation erodes purchasing power because the same amount of money buys fewer goods over time. Central banks raise interest rates to slow inflation by making borrowing more expensive."
  question: "Why do central banks raise interest rates in response to inflation, and what is the intended mechanism?"
  answer: "Central banks raise interest rates to make borrowing more expensive, which reduces spending and demand, slowing the rate at which prices increase and thus protecting purchasing power."
  confidence: 0.93

Example (bad -- too vague, no causal reasoning):
  question: "Why is inflation important?"
  answer: "Because it affects the economy."`
	},

	challenging: {
		cloze: `
CLOZE DELETIONS (CHALLENGING):
- Blank a phrase that requires multi-step reasoning or synthesis to recall
- The blanked content should test application of the concept, not just recognition
- Ensure the surrounding context provides enough clues for someone who deeply understands the material
- Format: "The {{c1::answer phrase}} is important because..."
- Only one cloze deletion per question

Example (good):
  Highlight: "The Dunning-Kruger effect describes how people with limited competence in a domain tend to overestimate their abilities because they lack the metacognitive skills to recognize their own incompetence."
  clozeText: "People with limited competence overestimate their abilities because they {{c1::lack the metacognitive skills to recognize their own incompetence}}."
  answer: "lack the metacognitive skills to recognize their own incompetence"
  confidence: 0.91`,

		definition: `
DEFINITION QUESTIONS (CHALLENGING):
- Require the reader to explain mechanisms, trade-offs, or implications
- Good formats: "Explain how X leads to Y", "What are the implications of X for Y?", "Compare X and Y in terms of their effect on Z"
- Answers should synthesize information and show reasoning chains

Example (good):
  Highlight: "Double-entry bookkeeping requires every transaction to be recorded in at least two accounts, maintaining the accounting equation: Assets = Liabilities + Equity."
  question: "Explain how double-entry bookkeeping maintains the integrity of financial records through its core constraint."
  answer: "By requiring every transaction to affect at least two accounts, double-entry bookkeeping ensures the accounting equation (Assets = Liabilities + Equity) always balances, making errors and fraud easier to detect since any imbalance signals a recording mistake."
  confidence: 0.88`,

		conceptual: `
CONCEPTUAL QUESTIONS (CHALLENGING):
- Require applying the concept to a novel scenario, analyzing trade-offs, or evaluating competing explanations
- Good formats: "If X were changed, how would Y be affected?", "Evaluate why approach A is preferred over B for Z", "Apply the principle of X to explain Y"
- Answers must build a reasoning chain with evidence from the highlight

Example (good):
  Highlight: "TCP uses a three-way handshake (SYN, SYN-ACK, ACK) to establish connections, ensuring both sides are ready to communicate before data transfer begins."
  question: "If TCP's three-way handshake were reduced to a two-step process, what reliability guarantees would be lost and why?"
  answer: "Without the third step (ACK), the server would have no confirmation that the client received its SYN-ACK. This means the server might allocate resources for a connection the client never acknowledged, leaving the server vulnerable to half-open connections and unable to verify mutual readiness before data transfer."
  confidence: 0.87`
	}
};

/**
 * Builds the complete prompt for question generation.
 *
 * @param highlights - Highlights to generate questions from
 * @param questionTypes - Types of questions to include
 * @param collection - Parent collection for context (title, author, sourceType)
 * @param context - Additional context: tags, card counts, difficulty
 * @returns Complete prompt string for the AI
 */
export function buildGenerationPrompt(
	highlights: Highlight[],
	questionTypes: QuestionType[],
	collection: Collection,
	context: PromptContext = {}
): string {
	const difficulty = context.difficulty ?? 'standard';
	const selectedInstructions = questionTypes
		.map((t) => typeInstructions[difficulty][t])
		.join('\n\n');

	const sourceLabel = formatSourceType(collection.sourceType);

	return `You are helping a reader retain knowledge from "${collection.title}"${collection.author ? ` by ${collection.author}` : ''} (${sourceLabel}).

Generate study questions from the following highlights. Create 1-3 questions per highlight based on how much meaningful content it contains.

${selectedInstructions}

QUALITY GUIDELINES:
- Questions should test genuine understanding, not trivial details
- Answers must be grounded in the highlight text (found in or directly implied)
- Skip highlights that are too vague or don't contain learnable content
- When a highlight has a user note, use it as a signal for what the reader found important
- Avoid generating questions redundant with existing cards for the same highlight
- Prefer cloze deletions when possible as they're most effective for memory

CONFIDENCE SCORING:
- 0.90-1.0: Tests deep understanding with a clear, unambiguous answer
- 0.75-0.89: Good question with minor ambiguity or moderate depth
- 0.60-0.74: Surface-level question or answer has some ambiguity
- Below 0.60: Do not include -- question is too vague or trivial

OUTPUT FORMAT (JSON object with "questions" array):
{
  "questions": [
    {
      "highlightId": "uuid",
      "questionType": "cloze" | "definition" | "conceptual",
      "question": "...",
      "answer": "...",
      "clozeText": "... {{c1::answer}} ..." (only for cloze type),
      "confidence": 0.85
    }
  ]
}

HIGHLIGHTS:
${highlights
	.map((h) => {
		const tags = context.highlightTags?.[h.id];
		const existingCards = context.existingCardCounts?.[h.id] ?? 0;

		return `
---
ID: ${h.id}
Text: "${h.text}"
${h.note ? `Note (reader's annotation): "${h.note}"` : ''}
${h.chapter ? `Chapter: ${h.chapter}` : ''}
${tags?.length ? `Tags: ${tags.join(', ')}` : ''}
${existingCards > 0 ? `Existing cards: ${existingCards} (avoid redundant questions)` : ''}
${h.contextBefore ? `Context before: "${h.contextBefore}"` : ''}
${h.contextAfter ? `Context after: "${h.contextAfter}"` : ''}`;
	})
	.join('\n')}

Generate questions now. Output ONLY the JSON object, no other text:`;
}

function formatSourceType(sourceType: SourceType): string {
	const labels: Record<SourceType, string> = {
		kindle: 'Kindle book',
		epub: 'ePub book',
		pdf: 'PDF document',
		web_article: 'web article',
		manual: 'manual entry'
	};
	return labels[sourceType] ?? sourceType;
}

/**
 * Builds message array for OpenAI Chat Completions API
 */
export function buildOpenAIMessages(prompt: string) {
	return [
		{
			role: 'system' as const,
			content: SYSTEM_MESSAGE
		},
		{
			role: 'user' as const,
			content: prompt
		}
	];
}

/**
 * Builds the system message string for Anthropic Messages API
 */
export function buildAnthropicSystem(): string {
	return SYSTEM_MESSAGE;
}

/**
 * Builds message array for Anthropic Messages API
 */
export function buildAnthropicMessages(prompt: string) {
	return [
		{
			role: 'user' as const,
			content: prompt
		}
	];
}

/**
 * Parses AI response content into structured questions
 *
 * Handles various response formats:
 * - Plain JSON array
 * - JSON object with "questions" array (from response_format: json_object)
 * - JSON wrapped in markdown code blocks
 *
 * @param content - Raw AI response content
 * @returns Array of valid generated questions
 */
export function parseGeneratedQuestions(content: string): GeneratedQuestion[] {
	let jsonStr = content.trim();

	// Handle markdown code block wrapper
	const jsonMatch = jsonStr.match(/```(?:json)?\s*([\s\S]*?)\s*```/);
	if (jsonMatch) {
		jsonStr = jsonMatch[1].trim();
	}

	const parsed = tryParseJSON(jsonStr);
	if (parsed === undefined) {
		if (content.length > 10) {
			console.warn('Failed to parse AI response:', content.slice(0, 200));
		}
		return [];
	}

	// Handle JSON object wrapper (OpenAI json_object mode returns {"questions": [...]})
	let questions: unknown[];
	if (Array.isArray(parsed)) {
		questions = parsed;
	} else if (parsed && typeof parsed === 'object') {
		questions = Array.isArray(parsed.questions)
			? parsed.questions
			: ((Object.values(parsed).find((v) => Array.isArray(v)) as unknown[]) ?? []);
	} else {
		return [];
	}

	// Filter to only valid questions with all required fields
	const valid = questions.filter((q: unknown): q is GeneratedQuestion => {
		const item = q as Record<string, unknown>;
		return (
			typeof item?.highlightId === 'string' &&
			typeof item?.questionType === 'string' &&
			typeof item?.question === 'string' &&
			typeof item?.answer === 'string' &&
			typeof item?.confidence === 'number'
		);
	});

	if (valid.length === 0 && content.length > 10) {
		console.warn('AI returned content but no valid questions were parsed:', content.slice(0, 200));
	}

	return valid;
}
